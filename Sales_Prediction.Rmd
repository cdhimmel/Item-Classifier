---
title: "Sales Predictor"
author: "Christopher Himmel"
date: "January 16, 2016"
output: word_document
---

#Introduction

A valuable exercise in retail is being able to predict sales of your inventory.  Predictive models can be used to do this, building a relationship betwen given information that you have currently and future sales.  Current information includes previous sales and Item attributes, of which there are many.

The many item attributes are recorded when the new SKU's are entered by hand one by one, or loaded as a group by uploading them and processing them into the system.  They are stored in the Inventory Master file (INVMST).  Sales history is loaded as stores sell inventory, quantities and prices are uploaded from the Point-Of-Sale and accumulated into a few buckets in the Inventory Balance files (INVBAL and INVCBL). 

The data used for this exercise is directly pulled out of JDA's Merchandise Management System, which runs on IBM's System-i.  The information is downloaded from INVMST, INVCBL and INVBAL (limited to only ISTORE between 1001 and 1003).

The following is the R code and results from this exercise.

#Set up R environment

```{r, message=FALSE}
library(tm)
library(dplyr)
library(wordcloud)
library(SnowballC)
library(corrplot)
library(rpart)
library(rattle)
library(randomForest)
library(tidyr)
library(ggplot2)
setwd("C:/Users/Christopher/Dropbox/Deep Learning/Projects/Sales-Prediction")
```

#Upload data pulled out of MMS

Load and view Item Master (INVMST):

```{r}
Item_Master = read.csv("data/INVMST - Item Master.csv")
#View(Item_Master)
#summary(Item_Master)
```

Load and view Chain Level Inventory Balance file (INVCBL):

```{r}
Inv_Bal_Chain = read.csv("data/INVCBL - Chain Level Inventory Balance Data.csv")
#View(Inv_Bal_Chain)
#summary(Inv_Bal_Chain)
```

Load and view Store Level Inventory Balance file, filtered for Niquea'D stores (INVBAL):

```{r}
Inv_Bal_Store = read.csv("data/INVBAL - Store Level Inventory Balance Data - Niquea'D.csv")
#View(Inv_Bal_Store)
#summary(Inv_Bal_Store)
```

#Format data set

Create chain level field with combined regular sales and advertised sales:

```{r}
Inv_Bal_Chain_mut <-
	Inv_Bal_Chain %>%
	mutate(chn_sales=CBRSUY+CBASUY)
```

Create total Niquea'D summarized sales with combined regular sales and advertised sales:

```{r}
Inv_Bal_Store_sum <- 
	Inv_Bal_Store %>%
	group_by(INUMBR) %>%
	summarise(nd_sales=sum(IBRSUY+IBASUY),
	          IBHAND_sum=sum(IBHAND),
	          IBWKCR_sum=sum(IBWKCR),
	          IBWK01_sum=sum(IBWK01),
	          IBWK02_sum=sum(IBWK02),
	          IBWK03_sum=sum(IBWK03),
	          IBWK04_sum=sum(IBWK04),
	          IBWK05_sum=sum(IBWK05),
	          IBWK06_sum=sum(IBWK06),
	          IBWK07_sum=sum(IBWK07),
	          IBWK08_sum=sum(IBWK08))
Inv_Bal_Store_sum <- subset(Inv_Bal_Store_sum, IBHAND_sum!=0 | nd_sales!=0)
```

Combine two new sales numbers into one file by SKU:

```{r}
Inv_values <-
	Inv_Bal_Store_sum %>%
	left_join(Inv_Bal_Chain_mut, by="INUMBR") %>%
	select(INUMBR,nd_sales,chn_sales,IBHAND_sum,IBWKCR_sum,IBWK01_sum,IBWK02_sum,IBWK03_sum,IBWK04_sum,
	       IBWK05_sum,IBWK06_sum,IBWK07_sum,IBWK08_sum)
```

Combine SKU, sales data into one file for analysis:

```{r}
Inv_final <-
	Inv_values %>%
	left_join(Item_Master, by="INUMBR")
```

#Break descriptions down for learning

Extract word list from description:

```{r}
word_list <- paste(Inv_final$IDESCR, collapse=" ")
word_list_vector <- VectorSource(word_list)
rm(word_list)
word_list_corpus <- Corpus(word_list_vector)
rm(word_list_vector)

word_list_corpus <- tm_map(word_list_corpus, content_transformer(tolower))
word_list_corpus <- tm_map(word_list_corpus, removePunctuation)
word_list_corpus <- tm_map(word_list_corpus, stripWhitespace)
word_list_corpus <- tm_map(word_list_corpus, removeNumbers)
word_list_corpus <- tm_map(word_list_corpus, removeWords, stopwords("english"))
word_list_corpus <- tm_map(word_list_corpus, stemDocument)

tdm<-TermDocumentMatrix(word_list_corpus)
rm(word_list_corpus)
tdm2<-as.matrix(tdm)
rm(tdm)
tdm3<-as.data.frame(tdm2)
rm(tdm2)
colnames(tdm3)<-c("count")
tdm4<-subset(tdm3,tdm3$count>132)
rm(tdm3)
top_words_list<-rownames(tdm4)
rm(tdm4)
```

Generate training vector:

```{r}
description_list <- Inv_final$IDESCR
description_list_vector <- VectorSource(description_list)
rm(description_list)
description_list_corpus <- Corpus(description_list_vector)
rm(description_list_vector)

description_list_corpus <- tm_map(description_list_corpus, content_transformer(tolower))
description_list_corpus <- tm_map(description_list_corpus, removePunctuation)
description_list_corpus <- tm_map(description_list_corpus, stripWhitespace)
description_list_corpus <- tm_map(description_list_corpus, removeNumbers)
description_list_corpus <- tm_map(description_list_corpus, removeWords, stopwords("english"))
description_list_corpus <- tm_map(description_list_corpus, stemDocument)

dtm_dataset<-DocumentTermMatrix(description_list_corpus)
rm(description_list_corpus)
dtm_dataset2<-as.matrix(dtm_dataset)
rm(dtm_dataset)
dtm_match<-match(top_words_list,colnames(dtm_dataset2))
dtm_dataset_top<-dtm_dataset2[,dtm_match]
rm(dtm_dataset2)
dtm_dataset_topdf<-as.data.frame(dtm_dataset_top)
rm(dtm_dataset_top)
colnames(dtm_dataset_topdf)<-paste("w", colnames(dtm_dataset_topdf), sep="_")
```

Add word values to training set:

```{r}
Inv_final$rownumber<-c(1:nrow(Inv_final))
dtm_dataset_topdf$rownumber<-rownames(dtm_dataset_topdf)
Inv_final<-merge(Inv_final,dtm_dataset_topdf,by="rownumber")
```

#Test each attribute for significant amount of data
(done in Preprocessing.rmd)

#Clean up certain missing values

```{r}
Inv_final$BYRNUM[is.na(Inv_final$BYRNUM)]<-0
Inv_final$ITKTTR[is.na(Inv_final$ITKTTR)]<-0
Inv_final$ILBLTR[is.na(Inv_final$ILBLTR)]<-0
Inv_final$IRPLCD[is.na(Inv_final$IRPLCD)]<-0
Inv_final$IWGHT[Inv_final$IWGHT==3001.4]=0
Inv_final$IDISTM[is.na(Inv_final$IDISTM)]<-0
```

#Check cross correlation of attributes to eliminate reduncancy

Create corrplot of Inventory Master (we see that ITKTTR, ITKTN and ILBLTR are highly correlated.  ITKTTR, it turns out is not a significant predictor of sales, so it will be removed.  ILBLTR will also be removed due to colinearity.)

```{r, echo=FALSE}
Inv_final_data<-Inv_final[c("ASNUM","IDEPT","ISDEPT","ICLAS","ISCLAS","IMFGNO","BYRNUM","ITKTTR","ITKTN","ILBLTR","IFINLN","ISTYPE","IMINPK","ISTDPK","IMXSTK","IDSPLY","IRPLCD","IWGHT","IVPLHI","IMDATE","IDISTM","ISEASN","INLRTL","IATRB3","IPRCCH","IPRCST","ICORGP","ILEAD","IMCRDT")]
Inv_final_num<-Inv_final_data[sapply(Inv_final_data, is.numeric)]
M<-cor(Inv_final_num)
corrplot(M,"color",title="Co-linearity Test")
```

#Build basis Linear Regression model

Linear Regression model (on Sales and Inventory only).
  R-squared: 0.3234

```{r, echo=FALSE}
model_sales=lm(IBWK01_sum~IBHAND_sum+IBWK02_sum+IBWK03_sum+IBWK04_sum+IBWK05_sum+IBWK06_sum+IBWK07_sum+IBWK08_sum, data=Inv_final)
summary(model_sales)
```

Complete working Linear Regression model (all non-empty attributes).
  R-squared: 0.4574

```{r, echo=FALSE}
model_attrib=lm(IBWK01_sum~IBHAND_sum+IBWK02_sum+IBWK03_sum+IBWK04_sum+IBWK05_sum+IBWK06_sum+IBWK07_sum+IBWK08_sum+ASNUM+IDEPT+ISDEPT+ICLAS+ISCLAS+IMFGNO+BYRNUM+ITKTTR+ITKTN+ILBLTR+IFINLN+ISTYPE+IMINPK+ISTDPK+IMXSTK+IDSPLY+IRPLCD+IWGHT+IVPLHI+IMDATE+IDISTM+ISEASN+INLRTL+IATRB3+IPRCCH+IPRCST+ICORGP+ILEAD+IMCRDT, data=Inv_final)
summary(model_attrib)
```

Add top 50 words to Linear Regression model.
  R-squared: 0.4586

```{r, echo=FALSE}
model_attrib=lm(IBWK01_sum~IBHAND_sum+IBWK02_sum+IBWK03_sum+IBWK04_sum+IBWK05_sum+IBWK06_sum+IBWK07_sum+IBWK08_sum+ASNUM+IDEPT+ISDEPT+ICLAS+ISCLAS+IMFGNO+BYRNUM+ITKTTR+ITKTN+ILBLTR+IFINLN+ISTYPE+IMINPK+ISTDPK+IMXSTK+IDSPLY+IRPLCD+IWGHT+IVPLHI+IMDATE+IDISTM+ISEASN+INLRTL+IATRB3+IPRCCH+IPRCST+ICORGP+ILEAD+IMCRDT+w_babi+w_bag+w_bdi+w_bird+w_birthday+w_black+w_blue+w_box+w_bracelet+w_butterfli+w_cake+w_candl+w_card+w_cardx+w_conv+w_crystal+w_dog+w_dress+w_earring+w_floral+w_flower+w_gem+w_general+w_girl+w_glass+w_glitter+w_gold+w_handmad+w_happi+w_heart+w_love+w_med+w_mini+w_mom+w_neck+w_necklac+w_note+w_pink+w_print+w_red+w_ribbon+w_set+w_silver+w_soap+w_thank+w_tree+w_vintag+w_wed+w_white+w_xbc, data=Inv_final)
summary(model_attrib)
```


Remove factor levels not correlated:

```{r}
Inv_final$IMFGNO[!(Inv_final$IMFGNO %in% c("4772", "4773", "4777", "9361", "9739"))]<-""
Inv_final$IFINLN[!(Inv_final$IFINLN %in% c("OTHER"))]<-""
Inv_final$ISTYPE[!(Inv_final$ISTYPE %in% c("05", "GW"))]<-""
Inv_final$ISEASN[!(Inv_final$ISEASN %in% c("HNK", "XMS"))]<-""
Inv_final$IATRB3[!(Inv_final$IATRB3 %in% c("ID", "JP", "SL", "TW"))]<-""
```


Minimal Linear Regression model (all non-empty attributes, removing non-correlated and colinear attributes, and non-correlated words)
  Attributes removed:
    ITKTTR,
    IWGHT,
    IMDATE,
    ILEAD,
    IMXSTK,
    IRPLCD,
    IPRCCH,
    IPRCST,
    ICORGP.
  Words kept:
    Black, Red, Ribbon, Silver, Tree
  R-squared: 0.3724

```{r, echo=FALSE}
model_minimal=lm(IBWK01_sum~IBHAND_sum+IBWK02_sum+IBWK03_sum+IBWK04_sum+IBWK05_sum+IBWK06_sum+IBWK07_sum+IBWK08_sum+ASNUM+IDEPT+ISDEPT+ICLAS+ISCLAS+IMFGNO+BYRNUM+ITKTN+ILBLTR+IFINLN+IMINPK+ISTDPK+IDSPLY+IVPLHI+IDISTM+ISEASN+INLRTL+IATRB3+IMCRDT+w_black+w_red+w_ribbon+w_silver+w_tree, data=Inv_final)
summary(model_minimal)
```


#Prepare data for model evaluation

Break data set up into Training and Test sets (70-30).

```{r}
set.seed(123)
index<-sample(1:nrow(Inv_final),size=0.7*nrow(Inv_final))

Inv_final_train<-Inv_final[index,]
Inv_final_test<-Inv_final[-index,]
```

Remove factor levels from test set not in training set.

```{r, echo=FALSE}
Inv_final_train$IMFGNO<-Inv_final_train$IMFGNO[,drop=TRUE]
Inv_final_train$IFINLN<-Inv_final_train$IFINLN[,drop=TRUE]
Inv_final_train$ISTYPE<-Inv_final_train$ISTYPE[,drop=TRUE]
Inv_final_train$ICORGP<-Inv_final_train$ICORGP[,drop=TRUE]
Inv_final_train$ISEASN<-Inv_final_train$ISEASN[,drop=TRUE]
Inv_final_train$IATRB3<-Inv_final_train$IATRB3[,drop=TRUE]
Inv_final_train$IPRCCH<-Inv_final_train$IPRCCH[,drop=TRUE]
Inv_final_train$IPRCST<-Inv_final_train$IPRCST[,drop=TRUE]

Inv_final_test$IMFGNO<-Inv_final_test$IMFGNO[,drop=TRUE]
Inv_final_test$IFINLN<-Inv_final_test$IFINLN[,drop=TRUE]
Inv_final_test$ISTYPE<-Inv_final_test$ISTYPE[,drop=TRUE]
Inv_final_test$ICORGP<-Inv_final_test$ICORGP[,drop=TRUE]
Inv_final_test$ISEASN<-Inv_final_test$ISEASN[,drop=TRUE]
Inv_final_test$IATRB3<-Inv_final_test$IATRB3[,drop=TRUE]
Inv_final_test$IPRCCH<-Inv_final_test$IPRCCH[,drop=TRUE]
Inv_final_test$IPRCST<-Inv_final_test$IPRCST[,drop=TRUE]

id <- which(!(Inv_final_test$IMFGNO %in% levels(Inv_final_train$IMFGNO)))
Inv_final_test$IMFGNO[id]<-""

id <- which(!(Inv_final_test$IFINLN %in% levels(Inv_final_train$IFINLN)))
#Inv_final_test$IFINLN[id]<-""

id <- which(!(Inv_final_test$ISTYPE %in% levels(Inv_final_train$ISTYPE)))
#Inv_final_test$ISTYPE[id]<-"01"

id <- which(!(Inv_final_test$ICORGP %in% levels(Inv_final_train$ICORGP)))
Inv_final_test$ICORGP[id]<-""

id <- which(!(Inv_final_test$ISEASN %in% levels(Inv_final_train$ISEASN)))
#Inv_final_test$ISEASN[id]<-""

id <- which(!(Inv_final_test$IATRB3 %in% levels(Inv_final_train$IATRB3)))
#Inv_final_test$IATRB3[id]<-""

id <- which(!(Inv_final_test$IPRCCH %in% levels(Inv_final_train$IPRCCH)))
#Inv_final_test$IPRCCH[id]<-""

id <- which(!(Inv_final_test$IPRCST %in% levels(Inv_final_train$IPRCST)))
#Inv_final_test$IPRCST[id]<-""

Inv_final_test$IMFGNO<-Inv_final_test$IMFGNO[,drop=TRUE]
Inv_final_test$IFINLN<-Inv_final_test$IFINLN[,drop=TRUE]
Inv_final_test$ISTYPE<-Inv_final_test$ISTYPE[,drop=TRUE]
Inv_final_test$ICORGP<-Inv_final_test$ICORGP[,drop=TRUE]
Inv_final_test$ISEASN<-Inv_final_test$ISEASN[,drop=TRUE]
Inv_final_test$IATRB3<-Inv_final_test$IATRB3[,drop=TRUE]
Inv_final_test$IPRCCH<-Inv_final_test$IPRCCH[,drop=TRUE]
Inv_final_test$IPRCST<-Inv_final_test$IPRCST[,drop=TRUE]

for (p in names(Inv_final_test)) { 
     if (class(Inv_final_train[[p]]) == "factor" & p!="ISTYPE") { 
         levels(Inv_final_test[[p]]) <- levels(Inv_final_train[[p]]) 
     } 
}
```

#Create models and evaluate

Calculate Baseline Model

```{r}
best.guess<-mean(Inv_final_train$IBWK01_sum)
RMSE.baseline<-sqrt(mean((best.guess-Inv_final_test$IBWK01_sum)^2))
message('RMSE: ', RMSE.baseline)
MAE.baseline<-mean(abs(best.guess-Inv_final_test$IBWK01_sum))
message('MAE: ', MAE.baseline)
```

Create minimal Linear Regression model on Training Data

```{r, echo=FALSE}
model_lr=lm(IBWK01_sum~IBHAND_sum+IBWK02_sum+IBWK03_sum+IBWK04_sum+IBWK05_sum+IBWK06_sum+IBWK07_sum+IBWK08_sum+ASNUM+IDEPT+ISDEPT+ICLAS+ISCLAS+IMFGNO+BYRNUM+ITKTN+ILBLTR+IFINLN+IMINPK+ISTDPK+IDSPLY+IVPLHI+IDISTM+ISEASN+INLRTL+IATRB3+IMCRDT+w_black+w_red+w_ribbon+w_silver+w_tree, data=Inv_final_train)
summary(model_lr)
```

Calculate errors on LR model

```{r, echo=FALSE}
test.pred.sales.lr<-predict(model_lr,Inv_final_test)
RMSE.lr<-sqrt(mean((test.pred.sales.lr-Inv_final_test$IBWK01_sum)^2))
message('RMSE: ', RMSE.lr)
MAE.lr<-mean(abs(test.pred.sales.lr-Inv_final_test$IBWK01_sum))
message('MAE: ', MAE.lr)
```

Create Decision Tree model

```{r, echo=FALSE}
model_dt=rpart(IBWK01_sum~IBHAND_sum+IBWK02_sum+IBWK03_sum+IBWK04_sum+IBWK05_sum+IBWK06_sum+IBWK07_sum+IBWK08_sum+ASNUM+IDEPT+ISDEPT+ICLAS+ISCLAS+IMFGNO+BYRNUM+ITKTN+ILBLTR+IFINLN+IMINPK+ISTDPK+IDSPLY+IVPLHI+IDISTM+ISEASN+INLRTL+IATRB3+IMCRDT+w_black+w_red+w_ribbon+w_silver+w_tree, data=Inv_final_train)
summary(model_dt)
```

Calculate errors on DT model

```{r, echo=FALSE}
test.pred.sales.dt<-predict(model_dt,Inv_final_test)
RMSE.dt<-sqrt(mean((test.pred.sales.dt-Inv_final_test$IBWK01_sum)^2))
message('RMSE: ', RMSE.dt)
MAE.dt<-mean(abs(test.pred.sales.dt-Inv_final_test$IBWK01_sum))
message('MAE: ', MAE.dt)
```

Prune resulting model

```{r, echo=FALSE}
printcp(model_dt)
min.xerror <- model_dt$cptable[which.min(model_dt$cptable[,"xerror"]),"CP"]
model_dt.pruned <- prune(model_dt,cp = min.xerror) 
```

Calculate errors on pruned DT model

```{r, echo=FALSE}
test.pred.sales.dt.p<-predict(model_dt.pruned,Inv_final_test)
RMSE.dt.pruned<-sqrt(mean((test.pred.sales.dt.p-Inv_final_test$IBWK01_sum)^2))
message('RMSE: ', RMSE.dt.pruned)
MAE.dt.pruned<-mean(abs(test.pred.sales.dt.p-Inv_final_test$IBWK01_sum))
message('MAE: ', MAE.dt.pruned)
```

Create Random Forest model

```{r}
model_rf=randomForest(IBWK01_sum~IBHAND_sum+IBWK02_sum+IBWK03_sum+IBWK04_sum+IBWK05_sum+IBWK06_sum+IBWK07_sum+IBWK08_sum+ASNUM+IDEPT+ISDEPT+ICLAS+ISCLAS+IMFGNO+BYRNUM+ITKTN+ILBLTR+IFINLN+IMINPK+ISTDPK+IDSPLY+IVPLHI+IDISTM+ISEASN+INLRTL+IATRB3+IMCRDT+w_black+w_red+w_ribbon+w_silver+w_tree, data=Inv_final_train,importance=TRUE, ntree=1000)
#summary(model_rf)

message('Minimum MSE tree count:', which.min(model_rf$mse))
imp <- as.data.frame(sort(importance(model_rf)[,1],decreasing = TRUE),optional = T)
names(imp) <- "% Inc MSE"
message('Importance of independent variables:')
imp
```

Calculate errors on RF model

```{r, echo=FALSE}
test.pred.sales.rf<-predict(model_rf,Inv_final_test)
RMSE.rf<-sqrt(mean((test.pred.sales.rf-Inv_final_test$IBWK01_sum)^2))
message('RMSE: ', RMSE.rf)
MAE.rf<-mean(abs(test.pred.sales.rf-Inv_final_test$IBWK01_sum))
message('MAE: ', MAE.rf)
```

#Putting it all together!

```{r, echo=FALSE}
accuracy <- data.frame(Method = c("Baseline","Linear Regression","Full tree","Pruned tree","Random forest"),
            RMSE = c(RMSE.baseline,RMSE.lr,RMSE.dt,RMSE.dt.pruned,RMSE.rf),
            MAE = c(MAE.baseline,MAE.lr,MAE.dt,MAE.dt.pruned,MAE.rf)) 
accuracy$RMSE <- round(accuracy$RMSE,2)
accuracy$MAE <- round(accuracy$MAE,2) 
accuracy

message('First few predictions:')

predictions<-data.frame(Actual=Inv_final_test$IBWK01_sum,
                        Baseline=round(best.guess,2),
                        LinReg=round(test.pred.sales.lr,2),
                        DecTree=round(test.pred.sales.dt,2),
                        DecTreePruned=round(test.pred.sales.dt.p,2),
                        RandForest=round(test.pred.sales.rf,2))
head(predictions)
all.predictions <- gather(predictions,key = model,value = predictions,2:6)

ggplot(data = all.predictions,aes(x = Actual, y = predictions)) + 
  geom_point(colour = "blue") + 
  geom_abline(intercept = 0, slope = 1, colour = "red") +
  geom_vline(xintercept = 23, colour = "green", linetype = "dashed") +
  facet_wrap(~ model,ncol = 2) + 
  coord_cartesian(xlim = c(0,15),ylim = c(0,15)) +
  ggtitle("Predicted vs. Actual, by model")

message('Sales Units range is cut off at 15')
```

#Summary

From RMSE and MAE measurements, it appears that the Random Forest model is the best, which is to be expected.  Looking at the graphs of Predicted vs. Actual, Random Forest looks to push closer to ideal.  More input is needed to get a better prediction of R-squared of 0.37.

However, before removing all of the non-correlated attributes and description words, the R-squared was 0.46, and the best RMSE was higher (~1.80) than this minimal model.

It is difficult guaging which of the independent variables are the most important, as the Linear Regression lowest t valued variables don't correspond to the lowest Random Forest MSE valued variables.  Perhaps developing the RF model on the full training set and then comparing to the LR model developed on the full training set.  That would give a better representation of the full story.